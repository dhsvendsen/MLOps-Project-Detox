{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import kaggle\n",
    "import zipfile\n",
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import torch\n",
    "from transformers import DistilBertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\rune7\\\\Documents\\\\GitHub\\\\MLOps-Project-Detox\\\\notebooks'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#unzip the data\n",
    "with zipfile.ZipFile('../data/jigsaw-toxic-comment-classification-challenge.zip', 'r') as zip_ref:\n",
    "    zip_ref.extractall('../data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "files = glob.glob('../data/*.zip')\n",
    "for file in files:\n",
    "    with zipfile.ZipFile(file, 'r') as zip_ref:\n",
    "        zip_ref.extractall('../data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('../data/train.csv')\n",
    "\n",
    "classes = ['toxic', 'severe_toxic', 'obsence', 'threat', 'insult', 'identity_hate']\n",
    "\n",
    "#make a new column that contains vectors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file vocab.txt from cache at C:\\Users\\rune7/.cache\\huggingface\\hub\\models--distilbert-base-uncased\\snapshots\\1c4513b2eedbda136f57676a34eea67aba266e5c\\vocab.txt\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at C:\\Users\\rune7/.cache\\huggingface\\hub\\models--distilbert-base-uncased\\snapshots\\1c4513b2eedbda136f57676a34eea67aba266e5c\\tokenizer_config.json\n",
      "loading configuration file config.json from cache at C:\\Users\\rune7/.cache\\huggingface\\hub\\models--distilbert-base-uncased\\snapshots\\1c4513b2eedbda136f57676a34eea67aba266e5c\\config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "model_inputs = tokenizer(train['comment_text'].tolist(), padding=\"max_length\", truncation=True, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00001cee341fdb12</td>\n",
       "      <td>Yo bitch Ja Rule is more succesful then you'll...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0000247867823ef7</td>\n",
       "      <td>== From RfC == \\n\\n The title is fine as it is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00013b17ad220c46</td>\n",
       "      <td>\" \\n\\n == Sources == \\n\\n * Zawe Ashton on Lap...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00017563c3f7919a</td>\n",
       "      <td>:If you have a look back at the source, the in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00017695ad8997eb</td>\n",
       "      <td>I don't anonymously edit articles at all.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153159</th>\n",
       "      <td>fffcd0960ee309b5</td>\n",
       "      <td>. \\n i totally agree, this stuff is nothing bu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153160</th>\n",
       "      <td>fffd7a9a6eb32c16</td>\n",
       "      <td>== Throw from out field to home plate. == \\n\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153161</th>\n",
       "      <td>fffda9e8d6fafa9e</td>\n",
       "      <td>\" \\n\\n == Okinotorishima categories == \\n\\n I ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153162</th>\n",
       "      <td>fffe8f1340a79fc2</td>\n",
       "      <td>\" \\n\\n == \"\"One of the founding nations of the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153163</th>\n",
       "      <td>ffffce3fb183ee80</td>\n",
       "      <td>\" \\n :::Stop already. Your bullshit is not wel...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>153164 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      id                                       comment_text\n",
       "0       00001cee341fdb12  Yo bitch Ja Rule is more succesful then you'll...\n",
       "1       0000247867823ef7  == From RfC == \\n\\n The title is fine as it is...\n",
       "2       00013b17ad220c46  \" \\n\\n == Sources == \\n\\n * Zawe Ashton on Lap...\n",
       "3       00017563c3f7919a  :If you have a look back at the source, the in...\n",
       "4       00017695ad8997eb          I don't anonymously edit articles at all.\n",
       "...                  ...                                                ...\n",
       "153159  fffcd0960ee309b5  . \\n i totally agree, this stuff is nothing bu...\n",
       "153160  fffd7a9a6eb32c16  == Throw from out field to home plate. == \\n\\n...\n",
       "153161  fffda9e8d6fafa9e  \" \\n\\n == Okinotorishima categories == \\n\\n I ...\n",
       "153162  fffe8f1340a79fc2  \" \\n\\n == \"\"One of the founding nations of the...\n",
       "153163  ffffce3fb183ee80  \" \\n :::Stop already. Your bullshit is not wel...\n",
       "\n",
       "[153164 rows x 2 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>153164.000000</td>\n",
       "      <td>153164.000000</td>\n",
       "      <td>153164.000000</td>\n",
       "      <td>153164.000000</td>\n",
       "      <td>153164.000000</td>\n",
       "      <td>153164.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.039761</td>\n",
       "      <td>0.002396</td>\n",
       "      <td>0.024098</td>\n",
       "      <td>0.001378</td>\n",
       "      <td>0.022375</td>\n",
       "      <td>0.004649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.195399</td>\n",
       "      <td>0.048892</td>\n",
       "      <td>0.153355</td>\n",
       "      <td>0.037091</td>\n",
       "      <td>0.147899</td>\n",
       "      <td>0.068022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               toxic   severe_toxic        obscene         threat  \\\n",
       "count  153164.000000  153164.000000  153164.000000  153164.000000   \n",
       "mean        0.039761       0.002396       0.024098       0.001378   \n",
       "std         0.195399       0.048892       0.153355       0.037091   \n",
       "min         0.000000       0.000000       0.000000       0.000000   \n",
       "25%         0.000000       0.000000       0.000000       0.000000   \n",
       "50%         0.000000       0.000000       0.000000       0.000000   \n",
       "75%         0.000000       0.000000       0.000000       0.000000   \n",
       "max         1.000000       1.000000       1.000000       1.000000   \n",
       "\n",
       "              insult  identity_hate  \n",
       "count  153164.000000  153164.000000  \n",
       "mean        0.022375       0.004649  \n",
       "std         0.147899       0.068022  \n",
       "min         0.000000       0.000000  \n",
       "25%         0.000000       0.000000  \n",
       "50%         0.000000       0.000000  \n",
       "75%         0.000000       0.000000  \n",
       "max         1.000000       1.000000  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#we make evaluation dataset from test.csv\n",
    "test = pd.read_csv('../data/test.csv')\n",
    "test_labels = pd.read_csv('../data/test_labels.csv')\n",
    "#replace -1 with 0 in test labels\n",
    "test_labels = test_labels.replace(-1, 0)\n",
    "test_labels.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "153164"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ArrowInvalid",
     "evalue": "Column 1 named text expected length 7 but got length 153164",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mArrowInvalid\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 17\u001b[0m\n\u001b[0;32m     11\u001b[0m X_test \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(test[\u001b[39m'\u001b[39m\u001b[39mcomment_text\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m     12\u001b[0m y_test \u001b[39m=\u001b[39m test_labels\n\u001b[0;32m     15\u001b[0m d \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m:Dataset\u001b[39m.\u001b[39mfrom_dict({\u001b[39m'\u001b[39m\u001b[39mlabel\u001b[39m\u001b[39m'\u001b[39m:y_train,\u001b[39m'\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m'\u001b[39m:X_train}),\n\u001b[0;32m     16\u001b[0m      \u001b[39m'\u001b[39m\u001b[39mval\u001b[39m\u001b[39m'\u001b[39m:Dataset\u001b[39m.\u001b[39mfrom_dict({\u001b[39m'\u001b[39m\u001b[39mlabel\u001b[39m\u001b[39m'\u001b[39m:y_val,\u001b[39m'\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m'\u001b[39m:X_val}),\n\u001b[1;32m---> 17\u001b[0m      \u001b[39m'\u001b[39m\u001b[39mtest\u001b[39m\u001b[39m'\u001b[39m:Dataset\u001b[39m.\u001b[39;49mfrom_dict({\u001b[39m'\u001b[39;49m\u001b[39mlabel\u001b[39;49m\u001b[39m'\u001b[39;49m:y_test,\u001b[39m'\u001b[39;49m\u001b[39mtext\u001b[39;49m\u001b[39m'\u001b[39;49m:X_test})\n\u001b[0;32m     18\u001b[0m      }\n\u001b[0;32m     20\u001b[0m DatasetDict(d)\n",
      "File \u001b[1;32mc:\\Users\\rune7\\anaconda3\\envs\\mlops\\lib\\site-packages\\datasets\\arrow_dataset.py:868\u001b[0m, in \u001b[0;36mDataset.from_dict\u001b[1;34m(cls, mapping, features, info, split)\u001b[0m\n\u001b[0;32m    863\u001b[0m     mapping \u001b[39m=\u001b[39m features\u001b[39m.\u001b[39mencode_batch(mapping)\n\u001b[0;32m    864\u001b[0m mapping \u001b[39m=\u001b[39m {\n\u001b[0;32m    865\u001b[0m     col: OptimizedTypedSequence(data, \u001b[39mtype\u001b[39m\u001b[39m=\u001b[39mfeatures[col] \u001b[39mif\u001b[39;00m features \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m, col\u001b[39m=\u001b[39mcol)\n\u001b[0;32m    866\u001b[0m     \u001b[39mfor\u001b[39;00m col, data \u001b[39min\u001b[39;00m mapping\u001b[39m.\u001b[39mitems()\n\u001b[0;32m    867\u001b[0m }\n\u001b[1;32m--> 868\u001b[0m pa_table \u001b[39m=\u001b[39m InMemoryTable\u001b[39m.\u001b[39;49mfrom_pydict(mapping\u001b[39m=\u001b[39;49mmapping)\n\u001b[0;32m    869\u001b[0m \u001b[39mif\u001b[39;00m info\u001b[39m.\u001b[39mfeatures \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    870\u001b[0m     info\u001b[39m.\u001b[39mfeatures \u001b[39m=\u001b[39m Features({col: ts\u001b[39m.\u001b[39mget_inferred_type() \u001b[39mfor\u001b[39;00m col, ts \u001b[39min\u001b[39;00m mapping\u001b[39m.\u001b[39mitems()})\n",
      "File \u001b[1;32mc:\\Users\\rune7\\anaconda3\\envs\\mlops\\lib\\site-packages\\datasets\\table.py:801\u001b[0m, in \u001b[0;36mInMemoryTable.from_pydict\u001b[1;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[0;32m    785\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[0;32m    786\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfrom_pydict\u001b[39m(\u001b[39mcls\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    787\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    788\u001b[0m \u001b[39m    Construct a Table from Arrow arrays or columns.\u001b[39;00m\n\u001b[0;32m    789\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    799\u001b[0m \u001b[39m        `datasets.table.Table`\u001b[39;00m\n\u001b[0;32m    800\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 801\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39m(pa\u001b[39m.\u001b[39;49mTable\u001b[39m.\u001b[39;49mfrom_pydict(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs))\n",
      "File \u001b[1;32mc:\\Users\\rune7\\anaconda3\\envs\\mlops\\lib\\site-packages\\pyarrow\\table.pxi:3643\u001b[0m, in \u001b[0;36mpyarrow.lib.Table.from_pydict\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\rune7\\anaconda3\\envs\\mlops\\lib\\site-packages\\pyarrow\\table.pxi:5170\u001b[0m, in \u001b[0;36mpyarrow.lib._from_pydict\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\rune7\\anaconda3\\envs\\mlops\\lib\\site-packages\\pyarrow\\table.pxi:3592\u001b[0m, in \u001b[0;36mpyarrow.lib.Table.from_arrays\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\rune7\\anaconda3\\envs\\mlops\\lib\\site-packages\\pyarrow\\table.pxi:2785\u001b[0m, in \u001b[0;36mpyarrow.lib.Table.validate\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\rune7\\anaconda3\\envs\\mlops\\lib\\site-packages\\pyarrow\\error.pxi:100\u001b[0m, in \u001b[0;36mpyarrow.lib.check_status\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mArrowInvalid\u001b[0m: Column 1 named text expected length 7 but got length 153164"
     ]
    }
   ],
   "source": [
    "from datasets.dataset_dict import DatasetDict\n",
    "from datasets import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "y = np.array(train.drop(['id', 'comment_text'], axis=1))\n",
    "X = np.array(train['comment_text'])\n",
    "\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "X_test = np.array(test['comment_text'])\n",
    "y_test = test_labels\n",
    "\n",
    "\n",
    "d = {'train':Dataset.from_dict({'label':y_train,'text':X_train}),\n",
    "     'val':Dataset.from_dict({'label':y_val,'text':X_val}),\n",
    "     'test':Dataset.from_dict({'label':y_test,'text':X_test})\n",
    "     }\n",
    "\n",
    "DatasetDict(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the indices to a file\n",
    "import pickle\n",
    "#with open('../data/indices.pkl', 'wb') as f:\n",
    "#    pickle.dump(indices, f)\n",
    "\n",
    "#load the indices\n",
    "with open('../data/indices.pkl', 'rb') as f:\n",
    "    indices = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#convert to torch dataset\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "class ToxicDataset(Dataset):\n",
    "    def __init__(self, indices, df):\n",
    "        self.df = df\n",
    "        self.text = indices\n",
    "        #label is all columns except id and comment_text\n",
    "        self.label = np.array(df.drop(['id', 'comment_text'], axis=1))\n",
    "        self.tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    def __getitem__(self, idx):\n",
    "        label = torch.tensor(self.label[idx], dtype=torch.float32)\n",
    "        #text = torch.tensor(self.text['input_ids'][idx], dtype=torch.long)\n",
    "        text = torch.tensor(self.text[\"input_ids\"][idx]).squeeze()\n",
    "        return text, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file vocab.txt from cache at C:\\Users\\rune7/.cache\\huggingface\\hub\\models--distilbert-base-uncased\\snapshots\\1c4513b2eedbda136f57676a34eea67aba266e5c\\vocab.txt\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at C:\\Users\\rune7/.cache\\huggingface\\hub\\models--distilbert-base-uncased\\snapshots\\1c4513b2eedbda136f57676a34eea67aba266e5c\\tokenizer_config.json\n",
      "loading configuration file config.json from cache at C:\\Users\\rune7/.cache\\huggingface\\hub\\models--distilbert-base-uncased\\snapshots\\1c4513b2eedbda136f57676a34eea67aba266e5c\\config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_dataset = ToxicDataset(indices, train)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file vocab.txt from cache at C:\\Users\\rune7/.cache\\huggingface\\hub\\models--distilbert-base-uncased\\snapshots\\1c4513b2eedbda136f57676a34eea67aba266e5c\\vocab.txt\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at C:\\Users\\rune7/.cache\\huggingface\\hub\\models--distilbert-base-uncased\\snapshots\\1c4513b2eedbda136f57676a34eea67aba266e5c\\tokenizer_config.json\n",
      "loading configuration file config.json from cache at C:\\Users\\rune7/.cache\\huggingface\\hub\\models--distilbert-base-uncased\\snapshots\\1c4513b2eedbda136f57676a34eea67aba266e5c\\config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at C:\\Users\\rune7/.cache\\huggingface\\hub\\models--distilbert-base-uncased-finetuned-sst-2-english\\snapshots\\bfdd146ea2b6807255b73527f1327ca12b6ed5c4\\config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-uncased-finetuned-sst-2-english\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"finetuning_task\": \"sst-2\",\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"NEGATIVE\",\n",
      "    \"1\": \"POSITIVE\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"NEGATIVE\": 0,\n",
      "    \"POSITIVE\": 1\n",
      "  },\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at C:\\Users\\rune7/.cache\\huggingface\\hub\\models--distilbert-base-uncased-finetuned-sst-2-english\\snapshots\\bfdd146ea2b6807255b73527f1327ca12b6ed5c4\\pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing DistilBertForSequenceClassification.\n",
      "\n",
      "All the weights of DistilBertForSequenceClassification were initialized from the model checkpoint at distilbert-base-uncased-finetuned-sst-2-english.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use DistilBertForSequenceClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4956abe2d4cf4b1fa3b871e2361d6275",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "147874952a78402190186324f8a87e7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file vocab.txt from cache at C:\\Users\\rune7/.cache\\huggingface\\hub\\models--distilbert-base-uncased\\snapshots\\1c4513b2eedbda136f57676a34eea67aba266e5c\\vocab.txt\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at C:\\Users\\rune7/.cache\\huggingface\\hub\\models--distilbert-base-uncased\\snapshots\\1c4513b2eedbda136f57676a34eea67aba266e5c\\tokenizer_config.json\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5ab9c58bc334bd1afa8b981fbc12169",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/483 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at C:\\Users\\rune7/.cache\\huggingface\\hub\\models--distilbert-base-uncased\\snapshots\\1c4513b2eedbda136f57676a34eea67aba266e5c\\config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "c:\\Users\\rune7\\anaconda3\\envs\\mlops\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 159571\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 29922\n",
      "  Number of trainable parameters = 66955010\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f3ea8e7208a4d08b01c61f9000d94c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/29922 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "vars() argument must have __dict__ attribute",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 21\u001b[0m\n\u001b[0;32m      3\u001b[0m training_args \u001b[39m=\u001b[39m TrainingArguments(\n\u001b[0;32m      4\u001b[0m     output_dir\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m./results\u001b[39m\u001b[39m'\u001b[39m,          \u001b[39m# output directory\u001b[39;00m\n\u001b[0;32m      5\u001b[0m     num_train_epochs\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m,              \u001b[39m# total # of training epochs\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     11\u001b[0m     logging_steps\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m,\n\u001b[0;32m     12\u001b[0m )\n\u001b[0;32m     14\u001b[0m trainer \u001b[39m=\u001b[39m Trainer(\n\u001b[0;32m     15\u001b[0m     model\u001b[39m=\u001b[39mmodel,                         \u001b[39m# the instantiated ðŸ¤— Transformers model to be trained\u001b[39;00m\n\u001b[0;32m     16\u001b[0m     args\u001b[39m=\u001b[39mtraining_args,                  \u001b[39m# training arguments, defined above\u001b[39;00m\n\u001b[0;32m     17\u001b[0m     train_dataset\u001b[39m=\u001b[39mtrain_dataset,         \u001b[39m# training dataset\u001b[39;00m\n\u001b[0;32m     18\u001b[0m     eval_dataset\u001b[39m=\u001b[39mtrain_dataset           \u001b[39m# evaluation dataset\u001b[39;00m\n\u001b[0;32m     19\u001b[0m )\n\u001b[1;32m---> 21\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain()\n",
      "File \u001b[1;32mc:\\Users\\rune7\\anaconda3\\envs\\mlops\\lib\\site-packages\\transformers\\trainer.py:1527\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_wrapped \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\n\u001b[0;32m   1524\u001b[0m inner_training_loop \u001b[39m=\u001b[39m find_executable_batch_size(\n\u001b[0;32m   1525\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inner_training_loop, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_train_batch_size, args\u001b[39m.\u001b[39mauto_find_batch_size\n\u001b[0;32m   1526\u001b[0m )\n\u001b[1;32m-> 1527\u001b[0m \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[0;32m   1528\u001b[0m     args\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m   1529\u001b[0m     resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint,\n\u001b[0;32m   1530\u001b[0m     trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[0;32m   1531\u001b[0m     ignore_keys_for_eval\u001b[39m=\u001b[39;49mignore_keys_for_eval,\n\u001b[0;32m   1532\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\rune7\\anaconda3\\envs\\mlops\\lib\\site-packages\\transformers\\trainer.py:1749\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   1746\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_load_rng_state(resume_from_checkpoint)\n\u001b[0;32m   1748\u001b[0m step \u001b[39m=\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m\n\u001b[1;32m-> 1749\u001b[0m \u001b[39mfor\u001b[39;00m step, inputs \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(epoch_iterator):\n\u001b[0;32m   1750\u001b[0m \n\u001b[0;32m   1751\u001b[0m     \u001b[39m# Skip past any already trained steps if resuming training\u001b[39;00m\n\u001b[0;32m   1752\u001b[0m     \u001b[39mif\u001b[39;00m steps_trained_in_current_epoch \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m   1753\u001b[0m         steps_trained_in_current_epoch \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\rune7\\anaconda3\\envs\\mlops\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:628\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    625\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    626\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    627\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 628\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    629\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    631\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    632\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\rune7\\anaconda3\\envs\\mlops\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:671\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    669\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    670\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 671\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    672\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m    673\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\rune7\\anaconda3\\envs\\mlops\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:61\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     60\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 61\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcollate_fn(data)\n",
      "File \u001b[1;32mc:\\Users\\rune7\\anaconda3\\envs\\mlops\\lib\\site-packages\\transformers\\trainer_utils.py:696\u001b[0m, in \u001b[0;36mRemoveColumnsCollator.__call__\u001b[1;34m(self, features)\u001b[0m\n\u001b[0;32m    694\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, features: List[\u001b[39mdict\u001b[39m]):\n\u001b[0;32m    695\u001b[0m     features \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_remove_columns(feature) \u001b[39mfor\u001b[39;00m feature \u001b[39min\u001b[39;00m features]\n\u001b[1;32m--> 696\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdata_collator(features)\n",
      "File \u001b[1;32mc:\\Users\\rune7\\anaconda3\\envs\\mlops\\lib\\site-packages\\transformers\\data\\data_collator.py:70\u001b[0m, in \u001b[0;36mdefault_data_collator\u001b[1;34m(features, return_tensors)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[39m# In this function we'll make the assumption that all `features` in the batch\u001b[39;00m\n\u001b[0;32m     65\u001b[0m \u001b[39m# have the same attributes.\u001b[39;00m\n\u001b[0;32m     66\u001b[0m \u001b[39m# So we will look at the first element as a proxy for what attributes exist\u001b[39;00m\n\u001b[0;32m     67\u001b[0m \u001b[39m# on the whole batch.\u001b[39;00m\n\u001b[0;32m     69\u001b[0m \u001b[39mif\u001b[39;00m return_tensors \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mreturn\u001b[39;00m torch_default_data_collator(features)\n\u001b[0;32m     71\u001b[0m \u001b[39melif\u001b[39;00m return_tensors \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtf\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mreturn\u001b[39;00m tf_default_data_collator(features)\n",
      "File \u001b[1;32mc:\\Users\\rune7\\anaconda3\\envs\\mlops\\lib\\site-packages\\transformers\\data\\data_collator.py:109\u001b[0m, in \u001b[0;36mtorch_default_data_collator\u001b[1;34m(features)\u001b[0m\n\u001b[0;32m    106\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[0;32m    108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(features[\u001b[39m0\u001b[39m], Mapping):\n\u001b[1;32m--> 109\u001b[0m     features \u001b[39m=\u001b[39m [\u001b[39mvars\u001b[39m(f) \u001b[39mfor\u001b[39;00m f \u001b[39min\u001b[39;00m features]\n\u001b[0;32m    110\u001b[0m first \u001b[39m=\u001b[39m features[\u001b[39m0\u001b[39m]\n\u001b[0;32m    111\u001b[0m batch \u001b[39m=\u001b[39m {}\n",
      "File \u001b[1;32mc:\\Users\\rune7\\anaconda3\\envs\\mlops\\lib\\site-packages\\transformers\\data\\data_collator.py:109\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    106\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[0;32m    108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(features[\u001b[39m0\u001b[39m], Mapping):\n\u001b[1;32m--> 109\u001b[0m     features \u001b[39m=\u001b[39m [\u001b[39mvars\u001b[39;49m(f) \u001b[39mfor\u001b[39;00m f \u001b[39min\u001b[39;00m features]\n\u001b[0;32m    110\u001b[0m first \u001b[39m=\u001b[39m features[\u001b[39m0\u001b[39m]\n\u001b[0;32m    111\u001b[0m batch \u001b[39m=\u001b[39m {}\n",
      "\u001b[1;31mTypeError\u001b[0m: vars() argument must have __dict__ attribute"
     ]
    }
   ],
   "source": [
    "#train the model\n",
    "from transformers import Trainer, TrainingArguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          # output directory\n",
    "    num_train_epochs=3,              # total # of training epochs\n",
    "    per_device_train_batch_size=16,  # batch size per device during training\n",
    "    per_device_eval_batch_size=64,   # batch size for evaluation\n",
    "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    logging_dir='./logs',            # directory for storing logs\n",
    "    logging_steps=10,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,                         # the instantiated ðŸ¤— Transformers model to be trained\n",
    "    args=training_args,                  # training arguments, defined above\n",
    "    train_dataset=train_dataset,         # training dataset\n",
    "    eval_dataset=train_dataset           # evaluation dataset\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlops",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0f87bbd9470326415a8cb0db74d1eb713136ba1ba202af67fbf5b56f8bdd7f3e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
