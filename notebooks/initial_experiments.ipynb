{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import kaggle\n",
    "import zipfile\n",
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import torch\n",
    "from transformers import DistilBertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\rune7\\\\Documents\\\\GitHub\\\\MLOps-Project-Detox\\\\notebooks'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#unzip the data\n",
    "with zipfile.ZipFile('../data/jigsaw-toxic-comment-classification-challenge.zip', 'r') as zip_ref:\n",
    "    zip_ref.extractall('../data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "files = glob.glob('../data/*.zip')\n",
    "for file in files:\n",
    "    with zipfile.ZipFile(file, 'r') as zip_ref:\n",
    "        zip_ref.extractall('../data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('../data/train.csv')\n",
    "\n",
    "classes = ['toxic', 'severe_toxic', 'obsence', 'threat', 'insult', 'identity_hate']\n",
    "\n",
    "#make a new column that contains vectors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m tokenizer \u001b[39m=\u001b[39m DistilBertTokenizer\u001b[39m.\u001b[39mfrom_pretrained(\u001b[39m'\u001b[39m\u001b[39mdistilbert-base-uncased\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m model_inputs \u001b[39m=\u001b[39m tokenizer(train[\u001b[39m'\u001b[39;49m\u001b[39mcomment_text\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39;49mtolist(), padding\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mmax_length\u001b[39;49m\u001b[39m\"\u001b[39;49m, truncation\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, return_tensors\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mpt\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\rune7\\anaconda3\\envs\\mlops\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2520\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.__call__\u001b[1;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m   2518\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_in_target_context_manager:\n\u001b[0;32m   2519\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_switch_to_input_mode()\n\u001b[1;32m-> 2520\u001b[0m     encodings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_one(text\u001b[39m=\u001b[39;49mtext, text_pair\u001b[39m=\u001b[39;49mtext_pair, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mall_kwargs)\n\u001b[0;32m   2521\u001b[0m \u001b[39mif\u001b[39;00m text_target \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   2522\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_switch_to_target_mode()\n",
      "File \u001b[1;32mc:\\Users\\rune7\\anaconda3\\envs\\mlops\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2606\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._call_one\u001b[1;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m   2601\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   2602\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbatch length of `text`: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(text)\u001b[39m}\u001b[39;00m\u001b[39m does not match batch length of `text_pair`:\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   2603\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(text_pair)\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   2604\u001b[0m         )\n\u001b[0;32m   2605\u001b[0m     batch_text_or_text_pairs \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mzip\u001b[39m(text, text_pair)) \u001b[39mif\u001b[39;00m text_pair \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m text\n\u001b[1;32m-> 2606\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbatch_encode_plus(\n\u001b[0;32m   2607\u001b[0m         batch_text_or_text_pairs\u001b[39m=\u001b[39;49mbatch_text_or_text_pairs,\n\u001b[0;32m   2608\u001b[0m         add_special_tokens\u001b[39m=\u001b[39;49madd_special_tokens,\n\u001b[0;32m   2609\u001b[0m         padding\u001b[39m=\u001b[39;49mpadding,\n\u001b[0;32m   2610\u001b[0m         truncation\u001b[39m=\u001b[39;49mtruncation,\n\u001b[0;32m   2611\u001b[0m         max_length\u001b[39m=\u001b[39;49mmax_length,\n\u001b[0;32m   2612\u001b[0m         stride\u001b[39m=\u001b[39;49mstride,\n\u001b[0;32m   2613\u001b[0m         is_split_into_words\u001b[39m=\u001b[39;49mis_split_into_words,\n\u001b[0;32m   2614\u001b[0m         pad_to_multiple_of\u001b[39m=\u001b[39;49mpad_to_multiple_of,\n\u001b[0;32m   2615\u001b[0m         return_tensors\u001b[39m=\u001b[39;49mreturn_tensors,\n\u001b[0;32m   2616\u001b[0m         return_token_type_ids\u001b[39m=\u001b[39;49mreturn_token_type_ids,\n\u001b[0;32m   2617\u001b[0m         return_attention_mask\u001b[39m=\u001b[39;49mreturn_attention_mask,\n\u001b[0;32m   2618\u001b[0m         return_overflowing_tokens\u001b[39m=\u001b[39;49mreturn_overflowing_tokens,\n\u001b[0;32m   2619\u001b[0m         return_special_tokens_mask\u001b[39m=\u001b[39;49mreturn_special_tokens_mask,\n\u001b[0;32m   2620\u001b[0m         return_offsets_mapping\u001b[39m=\u001b[39;49mreturn_offsets_mapping,\n\u001b[0;32m   2621\u001b[0m         return_length\u001b[39m=\u001b[39;49mreturn_length,\n\u001b[0;32m   2622\u001b[0m         verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[0;32m   2623\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[0;32m   2624\u001b[0m     )\n\u001b[0;32m   2625\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   2626\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencode_plus(\n\u001b[0;32m   2627\u001b[0m         text\u001b[39m=\u001b[39mtext,\n\u001b[0;32m   2628\u001b[0m         text_pair\u001b[39m=\u001b[39mtext_pair,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2644\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m   2645\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\rune7\\anaconda3\\envs\\mlops\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2797\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.batch_encode_plus\u001b[1;34m(self, batch_text_or_text_pairs, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m   2787\u001b[0m \u001b[39m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001b[39;00m\n\u001b[0;32m   2788\u001b[0m padding_strategy, truncation_strategy, max_length, kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_padding_truncation_strategies(\n\u001b[0;32m   2789\u001b[0m     padding\u001b[39m=\u001b[39mpadding,\n\u001b[0;32m   2790\u001b[0m     truncation\u001b[39m=\u001b[39mtruncation,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2794\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m   2795\u001b[0m )\n\u001b[1;32m-> 2797\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_batch_encode_plus(\n\u001b[0;32m   2798\u001b[0m     batch_text_or_text_pairs\u001b[39m=\u001b[39;49mbatch_text_or_text_pairs,\n\u001b[0;32m   2799\u001b[0m     add_special_tokens\u001b[39m=\u001b[39;49madd_special_tokens,\n\u001b[0;32m   2800\u001b[0m     padding_strategy\u001b[39m=\u001b[39;49mpadding_strategy,\n\u001b[0;32m   2801\u001b[0m     truncation_strategy\u001b[39m=\u001b[39;49mtruncation_strategy,\n\u001b[0;32m   2802\u001b[0m     max_length\u001b[39m=\u001b[39;49mmax_length,\n\u001b[0;32m   2803\u001b[0m     stride\u001b[39m=\u001b[39;49mstride,\n\u001b[0;32m   2804\u001b[0m     is_split_into_words\u001b[39m=\u001b[39;49mis_split_into_words,\n\u001b[0;32m   2805\u001b[0m     pad_to_multiple_of\u001b[39m=\u001b[39;49mpad_to_multiple_of,\n\u001b[0;32m   2806\u001b[0m     return_tensors\u001b[39m=\u001b[39;49mreturn_tensors,\n\u001b[0;32m   2807\u001b[0m     return_token_type_ids\u001b[39m=\u001b[39;49mreturn_token_type_ids,\n\u001b[0;32m   2808\u001b[0m     return_attention_mask\u001b[39m=\u001b[39;49mreturn_attention_mask,\n\u001b[0;32m   2809\u001b[0m     return_overflowing_tokens\u001b[39m=\u001b[39;49mreturn_overflowing_tokens,\n\u001b[0;32m   2810\u001b[0m     return_special_tokens_mask\u001b[39m=\u001b[39;49mreturn_special_tokens_mask,\n\u001b[0;32m   2811\u001b[0m     return_offsets_mapping\u001b[39m=\u001b[39;49mreturn_offsets_mapping,\n\u001b[0;32m   2812\u001b[0m     return_length\u001b[39m=\u001b[39;49mreturn_length,\n\u001b[0;32m   2813\u001b[0m     verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[0;32m   2814\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[0;32m   2815\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\rune7\\anaconda3\\envs\\mlops\\lib\\site-packages\\transformers\\tokenization_utils.py:733\u001b[0m, in \u001b[0;36mPreTrainedTokenizer._batch_encode_plus\u001b[1;34m(self, batch_text_or_text_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m    730\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    731\u001b[0m     ids, pair_ids \u001b[39m=\u001b[39m ids_or_pair_ids\n\u001b[1;32m--> 733\u001b[0m first_ids \u001b[39m=\u001b[39m get_input_ids(ids)\n\u001b[0;32m    734\u001b[0m second_ids \u001b[39m=\u001b[39m get_input_ids(pair_ids) \u001b[39mif\u001b[39;00m pair_ids \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    735\u001b[0m input_ids\u001b[39m.\u001b[39mappend((first_ids, second_ids))\n",
      "File \u001b[1;32mc:\\Users\\rune7\\anaconda3\\envs\\mlops\\lib\\site-packages\\transformers\\tokenization_utils.py:700\u001b[0m, in \u001b[0;36mPreTrainedTokenizer._batch_encode_plus.<locals>.get_input_ids\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m    698\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_input_ids\u001b[39m(text):\n\u001b[0;32m    699\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(text, \u001b[39mstr\u001b[39m):\n\u001b[1;32m--> 700\u001b[0m         tokens \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtokenize(text, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    701\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconvert_tokens_to_ids(tokens)\n\u001b[0;32m    702\u001b[0m     \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(text, (\u001b[39mlist\u001b[39m, \u001b[39mtuple\u001b[39m)) \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(text) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(text[\u001b[39m0\u001b[39m], \u001b[39mstr\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\rune7\\anaconda3\\envs\\mlops\\lib\\site-packages\\transformers\\tokenization_utils.py:547\u001b[0m, in \u001b[0;36mPreTrainedTokenizer.tokenize\u001b[1;34m(self, text, **kwargs)\u001b[0m\n\u001b[0;32m    545\u001b[0m         tokenized_text\u001b[39m.\u001b[39mappend(token)\n\u001b[0;32m    546\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 547\u001b[0m         tokenized_text\u001b[39m.\u001b[39mextend(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_tokenize(token))\n\u001b[0;32m    548\u001b[0m \u001b[39m# [\"This\", \" is\", \" something\", \"<special_token_1>\", \"else\"]\u001b[39;00m\n\u001b[0;32m    549\u001b[0m \u001b[39mreturn\u001b[39;00m tokenized_text\n",
      "File \u001b[1;32mc:\\Users\\rune7\\anaconda3\\envs\\mlops\\lib\\site-packages\\transformers\\models\\bert\\tokenization_bert.py:244\u001b[0m, in \u001b[0;36mBertTokenizer._tokenize\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    242\u001b[0m split_tokens \u001b[39m=\u001b[39m []\n\u001b[0;32m    243\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdo_basic_tokenize:\n\u001b[1;32m--> 244\u001b[0m     \u001b[39mfor\u001b[39;00m token \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbasic_tokenizer\u001b[39m.\u001b[39;49mtokenize(text, never_split\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mall_special_tokens):\n\u001b[0;32m    245\u001b[0m \n\u001b[0;32m    246\u001b[0m         \u001b[39m# If the token is part of the never_split set\u001b[39;00m\n\u001b[0;32m    247\u001b[0m         \u001b[39mif\u001b[39;00m token \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbasic_tokenizer\u001b[39m.\u001b[39mnever_split:\n\u001b[0;32m    248\u001b[0m             split_tokens\u001b[39m.\u001b[39mappend(token)\n",
      "File \u001b[1;32mc:\\Users\\rune7\\anaconda3\\envs\\mlops\\lib\\site-packages\\transformers\\models\\bert\\tokenization_bert.py:431\u001b[0m, in \u001b[0;36mBasicTokenizer.tokenize\u001b[1;34m(self, text, never_split)\u001b[0m\n\u001b[0;32m    429\u001b[0m         \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrip_accents:\n\u001b[0;32m    430\u001b[0m             token \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_run_strip_accents(token)\n\u001b[1;32m--> 431\u001b[0m     split_tokens\u001b[39m.\u001b[39mextend(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_split_on_punc(token, never_split))\n\u001b[0;32m    433\u001b[0m output_tokens \u001b[39m=\u001b[39m whitespace_tokenize(\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(split_tokens))\n\u001b[0;32m    434\u001b[0m \u001b[39mreturn\u001b[39;00m output_tokens\n",
      "File \u001b[1;32mc:\\Users\\rune7\\anaconda3\\envs\\mlops\\lib\\site-packages\\transformers\\models\\bert\\tokenization_bert.py:457\u001b[0m, in \u001b[0;36mBasicTokenizer._run_split_on_punc\u001b[1;34m(self, text, never_split)\u001b[0m\n\u001b[0;32m    455\u001b[0m \u001b[39mwhile\u001b[39;00m i \u001b[39m<\u001b[39m \u001b[39mlen\u001b[39m(chars):\n\u001b[0;32m    456\u001b[0m     char \u001b[39m=\u001b[39m chars[i]\n\u001b[1;32m--> 457\u001b[0m     \u001b[39mif\u001b[39;00m _is_punctuation(char):\n\u001b[0;32m    458\u001b[0m         output\u001b[39m.\u001b[39mappend([char])\n\u001b[0;32m    459\u001b[0m         start_new_word \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "model_inputs = tokenizer(train['comment_text'].tolist(), padding=\"max_length\", truncation=True, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00001cee341fdb12</td>\n",
       "      <td>Yo bitch Ja Rule is more succesful then you'll...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0000247867823ef7</td>\n",
       "      <td>== From RfC == \\n\\n The title is fine as it is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00013b17ad220c46</td>\n",
       "      <td>\" \\n\\n == Sources == \\n\\n * Zawe Ashton on Lap...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00017563c3f7919a</td>\n",
       "      <td>:If you have a look back at the source, the in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00017695ad8997eb</td>\n",
       "      <td>I don't anonymously edit articles at all.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153159</th>\n",
       "      <td>fffcd0960ee309b5</td>\n",
       "      <td>. \\n i totally agree, this stuff is nothing bu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153160</th>\n",
       "      <td>fffd7a9a6eb32c16</td>\n",
       "      <td>== Throw from out field to home plate. == \\n\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153161</th>\n",
       "      <td>fffda9e8d6fafa9e</td>\n",
       "      <td>\" \\n\\n == Okinotorishima categories == \\n\\n I ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153162</th>\n",
       "      <td>fffe8f1340a79fc2</td>\n",
       "      <td>\" \\n\\n == \"\"One of the founding nations of the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153163</th>\n",
       "      <td>ffffce3fb183ee80</td>\n",
       "      <td>\" \\n :::Stop already. Your bullshit is not wel...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>153164 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      id                                       comment_text\n",
       "0       00001cee341fdb12  Yo bitch Ja Rule is more succesful then you'll...\n",
       "1       0000247867823ef7  == From RfC == \\n\\n The title is fine as it is...\n",
       "2       00013b17ad220c46  \" \\n\\n == Sources == \\n\\n * Zawe Ashton on Lap...\n",
       "3       00017563c3f7919a  :If you have a look back at the source, the in...\n",
       "4       00017695ad8997eb          I don't anonymously edit articles at all.\n",
       "...                  ...                                                ...\n",
       "153159  fffcd0960ee309b5  . \\n i totally agree, this stuff is nothing bu...\n",
       "153160  fffd7a9a6eb32c16  == Throw from out field to home plate. == \\n\\n...\n",
       "153161  fffda9e8d6fafa9e  \" \\n\\n == Okinotorishima categories == \\n\\n I ...\n",
       "153162  fffe8f1340a79fc2  \" \\n\\n == \"\"One of the founding nations of the...\n",
       "153163  ffffce3fb183ee80  \" \\n :::Stop already. Your bullshit is not wel...\n",
       "\n",
       "[153164 rows x 2 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>153164.000000</td>\n",
       "      <td>153164.000000</td>\n",
       "      <td>153164.000000</td>\n",
       "      <td>153164.000000</td>\n",
       "      <td>153164.000000</td>\n",
       "      <td>153164.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.039761</td>\n",
       "      <td>0.002396</td>\n",
       "      <td>0.024098</td>\n",
       "      <td>0.001378</td>\n",
       "      <td>0.022375</td>\n",
       "      <td>0.004649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.195399</td>\n",
       "      <td>0.048892</td>\n",
       "      <td>0.153355</td>\n",
       "      <td>0.037091</td>\n",
       "      <td>0.147899</td>\n",
       "      <td>0.068022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               toxic   severe_toxic        obscene         threat  \\\n",
       "count  153164.000000  153164.000000  153164.000000  153164.000000   \n",
       "mean        0.039761       0.002396       0.024098       0.001378   \n",
       "std         0.195399       0.048892       0.153355       0.037091   \n",
       "min         0.000000       0.000000       0.000000       0.000000   \n",
       "25%         0.000000       0.000000       0.000000       0.000000   \n",
       "50%         0.000000       0.000000       0.000000       0.000000   \n",
       "75%         0.000000       0.000000       0.000000       0.000000   \n",
       "max         1.000000       1.000000       1.000000       1.000000   \n",
       "\n",
       "              insult  identity_hate  \n",
       "count  153164.000000  153164.000000  \n",
       "mean        0.022375       0.004649  \n",
       "std         0.147899       0.068022  \n",
       "min         0.000000       0.000000  \n",
       "25%         0.000000       0.000000  \n",
       "50%         0.000000       0.000000  \n",
       "75%         0.000000       0.000000  \n",
       "max         1.000000       1.000000  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#we make evaluation dataset from test.csv\n",
    "test = pd.read_csv('../data/test.csv')\n",
    "test_labels = pd.read_csv('../data/test_labels.csv')\n",
    "#replace -1 with 0 in test labels\n",
    "test_labels = test_labels.replace(-1, 0)\n",
    "test_labels.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00001cee341fdb12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0000247867823ef7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00013b17ad220c46</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00017563c3f7919a</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00017695ad8997eb</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153159</th>\n",
       "      <td>fffcd0960ee309b5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153160</th>\n",
       "      <td>fffd7a9a6eb32c16</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153161</th>\n",
       "      <td>fffda9e8d6fafa9e</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153162</th>\n",
       "      <td>fffe8f1340a79fc2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153163</th>\n",
       "      <td>ffffce3fb183ee80</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>153164 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      id  toxic  severe_toxic  obscene  threat  insult  \\\n",
       "0       00001cee341fdb12      0             0        0       0       0   \n",
       "1       0000247867823ef7      0             0        0       0       0   \n",
       "2       00013b17ad220c46      0             0        0       0       0   \n",
       "3       00017563c3f7919a      0             0        0       0       0   \n",
       "4       00017695ad8997eb      0             0        0       0       0   \n",
       "...                  ...    ...           ...      ...     ...     ...   \n",
       "153159  fffcd0960ee309b5      0             0        0       0       0   \n",
       "153160  fffd7a9a6eb32c16      0             0        0       0       0   \n",
       "153161  fffda9e8d6fafa9e      0             0        0       0       0   \n",
       "153162  fffe8f1340a79fc2      0             0        0       0       0   \n",
       "153163  ffffce3fb183ee80      0             0        0       0       0   \n",
       "\n",
       "        identity_hate  \n",
       "0                   0  \n",
       "1                   0  \n",
       "2                   0  \n",
       "3                   0  \n",
       "4                   0  \n",
       "...               ...  \n",
       "153159              0  \n",
       "153160              0  \n",
       "153161              0  \n",
       "153162              0  \n",
       "153163              0  \n",
       "\n",
       "[153164 rows x 7 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#convert test_labels to have\n",
    "test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets.dataset_dict import DatasetDict\n",
    "from datasets import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "#import classlabel\n",
    "from datasets.features import ClassLabel\n",
    "\n",
    "y = np.array(train.drop(['id', 'comment_text'], axis=1))\n",
    "X = np.array(train['comment_text'])\n",
    "\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "#convert y_train and y_val to ClassLabel\n",
    "y_train = y_train.astype('int')\n",
    "y_val = y_val.astype('int')\n",
    "\n",
    "X_test = np.array(test['comment_text'])\n",
    "y_test = np.array(test_labels.drop(['id'], axis=1))\n",
    "\n",
    "\n",
    "\n",
    "d = {'train':Dataset.from_dict({'label':y_train,'text':X_train}),\n",
    "     'val':Dataset.from_dict({'label':y_val,'text':X_val}),\n",
    "     'test':Dataset.from_dict({'label':y_test,'text':X_test})\n",
    "     }\n",
    "\n",
    "raw_dataset = DatasetDict(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "473bd014ee0144efb575d7e50f01a7ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/128 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c5f50c1328041c69eecfe322d3ad4e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "174be714f37f4dd4ba3606de94cd2a7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/154 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#We convert dataset to tokenized pytorch tensors\n",
    "from transformers import DistilBertTokenizerFast\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text'], truncation=True)\n",
    "\n",
    "tokenized_dataset = raw_dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['label', 'text', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 127656\n",
       "    })\n",
       "    val: Dataset({\n",
       "        features: ['label', 'text', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 31915\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['label', 'text', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 153164\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['label', 'text', 'input_ids', 'attention_mask'])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset['train'][0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#convert to torch dataset\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "class ToxicDataset(Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        self.df = dataset\n",
    "        self.label = self.df['label']\n",
    "        self.text = self.df['text']\n",
    "        self.input_ids = self.df['input_ids']\n",
    "        self.attention_mask = self.df['attention_mask']\n",
    "  \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    def __getitem__(self, idx):\n",
    "        #convert to torch tensor\n",
    "        input_ids = torch.tensor(self.input_ids[idx])\n",
    "        attention_mask = torch.tensor(self.attention_mask[idx])\n",
    "        label = torch.tensor(self.label[idx])\n",
    "        return input_ids, attention_mask, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_dataset = ToxicDataset(tokenized_dataset['train'])\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "checkpoint = \"bert-base-uncased\"\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=6)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4956abe2d4cf4b1fa3b871e2361d6275",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "147874952a78402190186324f8a87e7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file vocab.txt from cache at C:\\Users\\rune7/.cache\\huggingface\\hub\\models--distilbert-base-uncased\\snapshots\\1c4513b2eedbda136f57676a34eea67aba266e5c\\vocab.txt\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at C:\\Users\\rune7/.cache\\huggingface\\hub\\models--distilbert-base-uncased\\snapshots\\1c4513b2eedbda136f57676a34eea67aba266e5c\\tokenizer_config.json\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5ab9c58bc334bd1afa8b981fbc12169",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/483 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at C:\\Users\\rune7/.cache\\huggingface\\hub\\models--distilbert-base-uncased\\snapshots\\1c4513b2eedbda136f57676a34eea67aba266e5c\\config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Jan  6 16:56:45 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 527.37       Driver Version: 527.37       CUDA Version: 12.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ... WDDM  | 00000000:01:00.0 Off |                  N/A |\n",
      "| N/A   29C    P8     9W /  N/A |   1118MiB /  8192MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      4768      C   ...da3\\envs\\mlops\\python.exe    N/A      |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running training *****\n",
      "  Num examples = 127656\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 96\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 96\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 3990\n",
      "  Number of trainable parameters = 109486854\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b00b552493bf47269238b18580f8ad52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3990 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 1.12 GiB (GPU 0; 8.00 GiB total capacity; 6.74 GiB already allocated; 0 bytes free; 6.83 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 18\u001b[0m\n\u001b[0;32m      7\u001b[0m data_collator \u001b[39m=\u001b[39m DataCollatorWithPadding(tokenizer\u001b[39m=\u001b[39mtokenizer)\n\u001b[0;32m     10\u001b[0m trainer \u001b[39m=\u001b[39m Trainer(\n\u001b[0;32m     11\u001b[0m     model,\n\u001b[0;32m     12\u001b[0m     training_args,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     16\u001b[0m     tokenizer\u001b[39m=\u001b[39mtokenizer,\n\u001b[0;32m     17\u001b[0m )\n\u001b[1;32m---> 18\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain()\n",
      "File \u001b[1;32mc:\\Users\\rune7\\anaconda3\\envs\\mlops\\lib\\site-packages\\transformers\\trainer.py:1527\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_wrapped \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\n\u001b[0;32m   1524\u001b[0m inner_training_loop \u001b[39m=\u001b[39m find_executable_batch_size(\n\u001b[0;32m   1525\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inner_training_loop, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_train_batch_size, args\u001b[39m.\u001b[39mauto_find_batch_size\n\u001b[0;32m   1526\u001b[0m )\n\u001b[1;32m-> 1527\u001b[0m \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[0;32m   1528\u001b[0m     args\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m   1529\u001b[0m     resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint,\n\u001b[0;32m   1530\u001b[0m     trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[0;32m   1531\u001b[0m     ignore_keys_for_eval\u001b[39m=\u001b[39;49mignore_keys_for_eval,\n\u001b[0;32m   1532\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\rune7\\anaconda3\\envs\\mlops\\lib\\site-packages\\transformers\\trainer.py:1775\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   1773\u001b[0m         tr_loss_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining_step(model, inputs)\n\u001b[0;32m   1774\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1775\u001b[0m     tr_loss_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining_step(model, inputs)\n\u001b[0;32m   1777\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[0;32m   1778\u001b[0m     args\u001b[39m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   1779\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[0;32m   1780\u001b[0m     \u001b[39mand\u001b[39;00m (torch\u001b[39m.\u001b[39misnan(tr_loss_step) \u001b[39mor\u001b[39;00m torch\u001b[39m.\u001b[39misinf(tr_loss_step))\n\u001b[0;32m   1781\u001b[0m ):\n\u001b[0;32m   1782\u001b[0m     \u001b[39m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   1783\u001b[0m     tr_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m tr_loss \u001b[39m/\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mglobal_step \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[1;32mc:\\Users\\rune7\\anaconda3\\envs\\mlops\\lib\\site-packages\\transformers\\trainer.py:2523\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[1;34m(self, model, inputs)\u001b[0m\n\u001b[0;32m   2520\u001b[0m     \u001b[39mreturn\u001b[39;00m loss_mb\u001b[39m.\u001b[39mreduce_mean()\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m   2522\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompute_loss_context_manager():\n\u001b[1;32m-> 2523\u001b[0m     loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompute_loss(model, inputs)\n\u001b[0;32m   2525\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mn_gpu \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m   2526\u001b[0m     loss \u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mmean()  \u001b[39m# mean() to average on multi-gpu parallel training\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\rune7\\anaconda3\\envs\\mlops\\lib\\site-packages\\transformers\\trainer.py:2555\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[1;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[0;32m   2553\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   2554\u001b[0m     labels \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m-> 2555\u001b[0m outputs \u001b[39m=\u001b[39m model(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49minputs)\n\u001b[0;32m   2556\u001b[0m \u001b[39m# Save past state if it exists\u001b[39;00m\n\u001b[0;32m   2557\u001b[0m \u001b[39m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[0;32m   2558\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mpast_index \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\rune7\\anaconda3\\envs\\mlops\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\rune7\\anaconda3\\envs\\mlops\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1566\u001b[0m, in \u001b[0;36mBertForSequenceClassification.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1558\u001b[0m \u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[39mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[0;32m   1560\u001b[0m \u001b[39m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[0;32m   1561\u001b[0m \u001b[39m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[0;32m   1562\u001b[0m \u001b[39m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[0;32m   1563\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1564\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[1;32m-> 1566\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbert(\n\u001b[0;32m   1567\u001b[0m     input_ids,\n\u001b[0;32m   1568\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[0;32m   1569\u001b[0m     token_type_ids\u001b[39m=\u001b[39;49mtoken_type_ids,\n\u001b[0;32m   1570\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[0;32m   1571\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[0;32m   1572\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[0;32m   1573\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m   1574\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[0;32m   1575\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[0;32m   1576\u001b[0m )\n\u001b[0;32m   1578\u001b[0m pooled_output \u001b[39m=\u001b[39m outputs[\u001b[39m1\u001b[39m]\n\u001b[0;32m   1580\u001b[0m pooled_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(pooled_output)\n",
      "File \u001b[1;32mc:\\Users\\rune7\\anaconda3\\envs\\mlops\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\rune7\\anaconda3\\envs\\mlops\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1021\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1012\u001b[0m head_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_head_mask(head_mask, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mnum_hidden_layers)\n\u001b[0;32m   1014\u001b[0m embedding_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membeddings(\n\u001b[0;32m   1015\u001b[0m     input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[0;32m   1016\u001b[0m     position_ids\u001b[39m=\u001b[39mposition_ids,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1019\u001b[0m     past_key_values_length\u001b[39m=\u001b[39mpast_key_values_length,\n\u001b[0;32m   1020\u001b[0m )\n\u001b[1;32m-> 1021\u001b[0m encoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(\n\u001b[0;32m   1022\u001b[0m     embedding_output,\n\u001b[0;32m   1023\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mextended_attention_mask,\n\u001b[0;32m   1024\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[0;32m   1025\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[0;32m   1026\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_extended_attention_mask,\n\u001b[0;32m   1027\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[0;32m   1028\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[0;32m   1029\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m   1030\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[0;32m   1031\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[0;32m   1032\u001b[0m )\n\u001b[0;32m   1033\u001b[0m sequence_output \u001b[39m=\u001b[39m encoder_outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m   1034\u001b[0m pooled_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler(sequence_output) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\rune7\\anaconda3\\envs\\mlops\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\rune7\\anaconda3\\envs\\mlops\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:610\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    601\u001b[0m     layer_outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[0;32m    602\u001b[0m         create_custom_forward(layer_module),\n\u001b[0;32m    603\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    607\u001b[0m         encoder_attention_mask,\n\u001b[0;32m    608\u001b[0m     )\n\u001b[0;32m    609\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 610\u001b[0m     layer_outputs \u001b[39m=\u001b[39m layer_module(\n\u001b[0;32m    611\u001b[0m         hidden_states,\n\u001b[0;32m    612\u001b[0m         attention_mask,\n\u001b[0;32m    613\u001b[0m         layer_head_mask,\n\u001b[0;32m    614\u001b[0m         encoder_hidden_states,\n\u001b[0;32m    615\u001b[0m         encoder_attention_mask,\n\u001b[0;32m    616\u001b[0m         past_key_value,\n\u001b[0;32m    617\u001b[0m         output_attentions,\n\u001b[0;32m    618\u001b[0m     )\n\u001b[0;32m    620\u001b[0m hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m    621\u001b[0m \u001b[39mif\u001b[39;00m use_cache:\n",
      "File \u001b[1;32mc:\\Users\\rune7\\anaconda3\\envs\\mlops\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\rune7\\anaconda3\\envs\\mlops\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:496\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    484\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[0;32m    485\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    486\u001b[0m     hidden_states: torch\u001b[39m.\u001b[39mTensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    493\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[torch\u001b[39m.\u001b[39mTensor]:\n\u001b[0;32m    494\u001b[0m     \u001b[39m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[0;32m    495\u001b[0m     self_attn_past_key_value \u001b[39m=\u001b[39m past_key_value[:\u001b[39m2\u001b[39m] \u001b[39mif\u001b[39;00m past_key_value \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m--> 496\u001b[0m     self_attention_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattention(\n\u001b[0;32m    497\u001b[0m         hidden_states,\n\u001b[0;32m    498\u001b[0m         attention_mask,\n\u001b[0;32m    499\u001b[0m         head_mask,\n\u001b[0;32m    500\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m    501\u001b[0m         past_key_value\u001b[39m=\u001b[39;49mself_attn_past_key_value,\n\u001b[0;32m    502\u001b[0m     )\n\u001b[0;32m    503\u001b[0m     attention_output \u001b[39m=\u001b[39m self_attention_outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m    505\u001b[0m     \u001b[39m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\rune7\\anaconda3\\envs\\mlops\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\rune7\\anaconda3\\envs\\mlops\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:426\u001b[0m, in \u001b[0;36mBertAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    416\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[0;32m    417\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    418\u001b[0m     hidden_states: torch\u001b[39m.\u001b[39mTensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    424\u001b[0m     output_attentions: Optional[\u001b[39mbool\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    425\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[torch\u001b[39m.\u001b[39mTensor]:\n\u001b[1;32m--> 426\u001b[0m     self_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mself(\n\u001b[0;32m    427\u001b[0m         hidden_states,\n\u001b[0;32m    428\u001b[0m         attention_mask,\n\u001b[0;32m    429\u001b[0m         head_mask,\n\u001b[0;32m    430\u001b[0m         encoder_hidden_states,\n\u001b[0;32m    431\u001b[0m         encoder_attention_mask,\n\u001b[0;32m    432\u001b[0m         past_key_value,\n\u001b[0;32m    433\u001b[0m         output_attentions,\n\u001b[0;32m    434\u001b[0m     )\n\u001b[0;32m    435\u001b[0m     attention_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput(self_outputs[\u001b[39m0\u001b[39m], hidden_states)\n\u001b[0;32m    436\u001b[0m     outputs \u001b[39m=\u001b[39m (attention_output,) \u001b[39m+\u001b[39m self_outputs[\u001b[39m1\u001b[39m:]  \u001b[39m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\rune7\\anaconda3\\envs\\mlops\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\rune7\\anaconda3\\envs\\mlops\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:348\u001b[0m, in \u001b[0;36mBertSelfAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    345\u001b[0m         relative_position_scores_key \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39meinsum(\u001b[39m\"\u001b[39m\u001b[39mbhrd,lrd->bhlr\u001b[39m\u001b[39m\"\u001b[39m, key_layer, positional_embedding)\n\u001b[0;32m    346\u001b[0m         attention_scores \u001b[39m=\u001b[39m attention_scores \u001b[39m+\u001b[39m relative_position_scores_query \u001b[39m+\u001b[39m relative_position_scores_key\n\u001b[1;32m--> 348\u001b[0m attention_scores \u001b[39m=\u001b[39m attention_scores \u001b[39m/\u001b[39;49m math\u001b[39m.\u001b[39;49msqrt(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattention_head_size)\n\u001b[0;32m    349\u001b[0m \u001b[39mif\u001b[39;00m attention_mask \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    350\u001b[0m     \u001b[39m# Apply the attention mask is (precomputed for all layers in BertModel forward() function)\u001b[39;00m\n\u001b[0;32m    351\u001b[0m     attention_scores \u001b[39m=\u001b[39m attention_scores \u001b[39m+\u001b[39m attention_mask\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 1.12 GiB (GPU 0; 8.00 GiB total capacity; 6.74 GiB already allocated; 0 bytes free; 6.83 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-06 17:02:07,899 WARNING Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x000001F5B4C438B0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed')': /api/5288891/envelope/\n",
      "2023-01-06 17:02:19,432 WARNING Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x000001F5B4C43B80>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed')': /api/5288891/envelope/\n",
      "2023-01-06 18:34:51,859 WARNING Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x000001F5B4CE42E0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed')': /api/5288891/envelope/\n"
     ]
    }
   ],
   "source": [
    "#train the model\n",
    "from transformers import Trainer, TrainingArguments, DataCollatorWithPadding\n",
    "\n",
    "training_args = TrainingArguments(\"test-trainer\", per_device_train_batch_size=96, per_device_eval_batch_size=96)\n",
    "\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"val\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlops",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15 (default, Nov 24 2022, 14:38:14) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0f87bbd9470326415a8cb0db74d1eb713136ba1ba202af67fbf5b56f8bdd7f3e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
